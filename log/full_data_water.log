Namespace(alpha_init=0.0, batch_norm=False, batch_size=512, cuda=True, data_dir='./data/SWaT_Dataset_Attack_v0.csv', graph='None', h_tol=0.0001, hidden_size=32, lambda1=0.0, log_interval=5, lr=0.002, max_iter=20, model='None', n_blocks=1, n_components=1, n_epochs=1, n_hidden=1, name='GANF_Water', output_dir='./checkpoint/model', rho_init=1.0, rho_max=1e+16, seed=18, weight_decay=0.0005)
Loading dataset
train_water.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  A = torch.tensor(init, requires_grad=True, device=device)
Params:
            n_blocks 1, 
            input_size 1, 
            hidden_size 32, 
            n_hidden 1, 
            cond_label_size=32, 
            batch_norm=False
            
GANF(
  (rnn): LSTM(1, 32, batch_first=True)
  (gcn): GNN(
    (lin_n): Linear(in_features=32, out_features=32, bias=True)
    (lin_r): Linear(in_features=32, out_features=32, bias=False)
    (lin_2): Linear(in_features=32, out_features=32, bias=True)
  )
  (nf): MAF(
    (net): FlowSequential(
      (0): MADE(
        (net_input): MaskedLinear(in_features=1, out_features=32, bias=True, cond_features=32)
        (net): Sequential(
          (0): Tanh()
          (1): MaskedLinear(in_features=32, out_features=32, bias=True)
          (2): Tanh()
          (3): MaskedLinear(in_features=32, out_features=2, bias=True)
        )
      )
    )
  )
)
12.157825 -3.6808941 151.31618 -3.6825318
Epoch: 1, train -log_prob: -1.76, test -log_prob: -3.38, roc_val: 0.7106, roc_test: 0.8913 ,h: 0.09849166870117188
rho: 1.0, alpha 0.0, h 0.09849166870117188
===========================================
5.1882133 -3.3029175 130.00385 -3.3074312
Epoch: 2, train -log_prob: 4.24, test -log_prob: -3.07, roc_val: 0.6813, roc_test: 0.8809 ,h: 0.049762725830078125
rho: 1.0, alpha 0.09849166870117188, h 0.049762725830078125
===========================================
4.5545797 -2.915989 133.2946 -2.9232647
Epoch: 3, train -log_prob: 0.51, test -log_prob: -2.72, roc_val: 0.7132, roc_test: 0.8882 ,h: 0.015811920166015625
rho: 10.0, alpha 0.09849166870117188, h 0.015811920166015625
===========================================
2.5407531 -3.3653164 146.86407 -3.3653164
Epoch: 4, train -log_prob: -2.04, test -log_prob: -3.16, roc_val: 0.6780, roc_test: 0.8847 ,h: 0.005336761474609375
rho: 10.0, alpha 0.2566108703613281, h 0.005336761474609375
===========================================
0.88151455 -3.495542 158.20822 -3.496608
Epoch: 5, train -log_prob: -0.77, test -log_prob: -3.29, roc_val: 0.6871, roc_test: 0.8871 ,h: 0.001621246337890625
rho: 10.0, alpha 0.3099784851074219, h 0.001621246337890625
===========================================
0.893501 -3.4604232 172.77766 -3.4657574
Epoch: 6, train -log_prob: -0.22, test -log_prob: -3.27, roc_val: 0.7175, roc_test: 0.8862 ,h: 0.0033111572265625
rho: 10.0, alpha 0.3261909484863281, h 0.0033111572265625
===========================================
1.7388256 -2.1019995 136.85205 -2.1393776
Epoch: 7, train -log_prob: 0.06, test -log_prob: -1.97, roc_val: 0.7191, roc_test: 0.8951 ,h: 0.00592041015625
rho: 100.0, alpha 0.3261909484863281, h 0.00592041015625
===========================================
-0.8723913 -4.0866165 163.285 -4.088526
Epoch: 8, train -log_prob: -2.86, test -log_prob: -3.89, roc_val: 0.7106, roc_test: 0.8858 ,h: 0.00098419189453125
rho: 1000.0, alpha 0.3261909484863281, h 0.00098419189453125
===========================================
0.018991608 -3.2864609 146.09363 -3.2864609
Epoch: 9, train -log_prob: 7.61, test -log_prob: -3.10, roc_val: 0.6964, roc_test: 0.8867 ,h: 0.00127410888671875
rho: 10000.0, alpha 0.3261909484863281, h 0.00127410888671875
===========================================
-0.38409388 -3.5389175 135.21817 -3.5476236
Epoch: 10, train -log_prob: -1.84, test -log_prob: -3.38, roc_val: 0.7270, roc_test: 0.8908 ,h: 0.00023651123046875
rho: 100000.0, alpha 0.3261909484863281, h 0.00023651123046875
===========================================
-0.04637465 -3.5083687 156.1006 -3.5116756
Epoch: 11, train -log_prob: -0.07, test -log_prob: -3.35, roc_val: 0.7443, roc_test: 0.8909 ,h: 0.000156402587890625
rho: 100000.0, alpha 23.977313995361328, h 0.000156402587890625
===========================================
2.0105853 -2.1351044 144.34846 -2.1351044
Epoch: 12, train -log_prob: 0.15, test -log_prob: -2.02, roc_val: 0.7311, roc_test: 0.8872 ,h: 3.814697265625e-05
rho: 1000000.0, alpha 23.977313995361328, h 3.814697265625e-05
===========================================
Epoch: 13, train -log_prob: -2.33, test -log_prob: -2.94, roc_val: 0.7452, roc_test: 0.8924 ,h: 7.62939453125e-06
save model 13 epoch
Epoch: 14, train -log_prob: -2.87, test -log_prob: -2.56, roc_val: 0.8105, roc_test: 0.8943 ,h: 0.0
Epoch: 15, train -log_prob: -2.92, test -log_prob: -3.24, roc_val: 0.7412, roc_test: 0.8919 ,h: 0.0
save model 15 epoch
Epoch: 16, train -log_prob: -2.96, test -log_prob: -3.08, roc_val: 0.7307, roc_test: 0.8916 ,h: 0.0
Epoch: 17, train -log_prob: -2.98, test -log_prob: -2.67, roc_val: 0.7630, roc_test: 0.8911 ,h: 0.0
Epoch: 18, train -log_prob: -2.99, test -log_prob: -3.31, roc_val: 0.7246, roc_test: 0.8910 ,h: 0.0
save model 18 epoch
Epoch: 19, train -log_prob: -3.02, test -log_prob: -3.12, roc_val: 0.7514, roc_test: 0.8922 ,h: 0.0
Epoch: 20, train -log_prob: -3.03, test -log_prob: -2.67, roc_val: 0.7901, roc_test: 0.8929 ,h: 0.0
Epoch: 21, train -log_prob: -3.04, test -log_prob: -3.33, roc_val: 0.7358, roc_test: 0.8902 ,h: 0.0
save model 21 epoch
Epoch: 22, train -log_prob: -3.05, test -log_prob: -3.18, roc_val: 0.7556, roc_test: 0.8932 ,h: 0.0
Epoch: 23, train -log_prob: -3.06, test -log_prob: -2.73, roc_val: 0.7633, roc_test: 0.8874 ,h: 0.0
Epoch: 24, train -log_prob: -3.07, test -log_prob: -3.37, roc_val: 0.7219, roc_test: 0.8889 ,h: 0.0
save model 24 epoch
Epoch: 25, train -log_prob: -3.09, test -log_prob: -3.18, roc_val: 0.7773, roc_test: 0.8933 ,h: 0.0
Epoch: 26, train -log_prob: -3.09, test -log_prob: -2.75, roc_val: 0.7935, roc_test: 0.8935 ,h: 0.0
Epoch: 27, train -log_prob: -3.09, test -log_prob: -3.40, roc_val: 0.7654, roc_test: 0.8951 ,h: 0.0
save model 27 epoch
Epoch: 28, train -log_prob: -3.11, test -log_prob: -3.21, roc_val: 0.7394, roc_test: 0.8902 ,h: 0.0
Epoch: 29, train -log_prob: -3.11, test -log_prob: -2.82, roc_val: 0.8042, roc_test: 0.8878 ,h: 0.0
Epoch: 30, train -log_prob: -3.11, test -log_prob: -3.41, roc_val: 0.7369, roc_test: 0.8895 ,h: 0.0
save model 30 epoch
Epoch: 31, train -log_prob: -3.12, test -log_prob: -3.24, roc_val: 0.8220, roc_test: 0.8985 ,h: 0.0
Epoch: 32, train -log_prob: -3.13, test -log_prob: -2.77, roc_val: 0.8127, roc_test: 0.8905 ,h: 0.0
Epoch: 33, train -log_prob: -3.13, test -log_prob: -3.43, roc_val: 0.7885, roc_test: 0.8944 ,h: 0.0
save model 33 epoch
Epoch: 34, train -log_prob: -3.14, test -log_prob: -3.25, roc_val: 0.7399, roc_test: 0.8922 ,h: 0.0
Epoch: 35, train -log_prob: -3.14, test -log_prob: -2.79, roc_val: 0.7524, roc_test: 0.8908 ,h: 0.0
Epoch: 36, train -log_prob: -3.14, test -log_prob: -3.45, roc_val: 0.7823, roc_test: 0.8936 ,h: 0.0
save model 36 epoch
Epoch: 37, train -log_prob: -3.15, test -log_prob: -3.26, roc_val: 0.7899, roc_test: 0.8902 ,h: 0.0
Epoch: 38, train -log_prob: -3.15, test -log_prob: -2.79, roc_val: 0.8530, roc_test: 0.8859 ,h: 0.0
Epoch: 39, train -log_prob: -3.15, test -log_prob: -3.45, roc_val: 0.7855, roc_test: 0.8901 ,h: 0.0
save model 39 epoch
Epoch: 40, train -log_prob: -3.16, test -log_prob: -3.27, roc_val: 0.7394, roc_test: 0.8914 ,h: 0.0
Epoch: 41, train -log_prob: -3.16, test -log_prob: -2.83, roc_val: 0.7806, roc_test: 0.8865 ,h: 0.0
Epoch: 42, train -log_prob: -3.16, test -log_prob: -3.46, roc_val: 0.7547, roc_test: 0.8915 ,h: 0.0
save model 42 epoch
(GANF) 